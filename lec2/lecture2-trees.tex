\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{etex}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{url}
\usepackage{moreverb}
\usepackage{fancyvrb}
\usepackage{natbib}
\usepackage{eulervm}
\usepackage{auto-pst-pdf}
\usepackage{pst-plot}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{algpseudocode}
\usepackage{multirow}

% Code snippets
\usepackage{minted}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
\newminted{python}{bgcolor=bgcolor}

% Checked marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Colors
\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

% Beamer layout
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

% Front slide
\title{{\bf Supervised Learning}\\
Tree-based methods}
\author{
Gilles Louppe (\href{https://twitter.com/glouppe}{@glouppe})
}
\date{October 17, 2016}

% Argmax
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}



% Decision trees ======================================================

\AtBeginSection[]
{
\begin{frame}[noframenumbering, plain]
  \frametitle{Outline}
  \tableofcontents[currentsection]
  % Die Option [pausesections]
\end{frame}
}

\section{Decision trees}

\begin{frame}
    \frametitle{Problem statement}

    \begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{./figures/classification.pdf}
    \includegraphics[width=0.45\textwidth]{./figures/regression.pdf}\\
    Classification vs. regression
    \end{figure}
\end{frame}


\begin{frame}
    \frametitle{Running example}

    \begin{columns}
    \begin{column}{0.5\textwidth}

    \begin{center}
    From {\bf \X{physicochemical properties}} (alcohol, acidity, sulphates, ...),

    \vspace{1cm}
    learn a {\bf \model{model}}
    \vspace{1cm}

    to predict {\bf \y{wine taste preferences}}.

    \end{center}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
      \vspace{-0.5cm}
      \includegraphics[scale=0.6]{./figures/wine.jpg}
      \end{figure}
    \end{column}
    \end{columns}
\end{frame}



\begin{frame}[fragile]
    \frametitle{Supervised learning}

    \begin{itemize}
    \item Data comes as a finite learning set ${\cal L} = \texttt{(\X{X}, \y{y})}$ where
        \begin{itemize}
            \item \X{Input samples} are given as an array of shape \texttt{(n\_samples, n\_features)}\\
            \vspace{0.25cm}
            E.g., feature values for wine physicochemical properties:
\begin{verbatim}
# fixed acidity, volatile acidity, ...
X = [[  7.4    0.     ...   0.56   9.4    0.  ]
     [  7.8    0.     ...   0.68   9.8    0.  ]
                      ...
     [  7.8    0.04   ...   0.65   9.8    0.  ]]
\end{verbatim}
\vspace{0.25cm}

            \item \y{Output values} are given as an array of shape \texttt{(n\_samples,)}\\
            \vspace{0.25cm}
            E.g., wine taste preferences (from 0 to 10):
\begin{verbatim}
y = [5 5 5 ... 6 7 6]
\end{verbatim}
        \end{itemize}

    \vspace{0.25cm}

    \item The goal is to build an estimator $\model{f}: \X{{\cal X}} \mapsto \y{{\cal Y}}$ minimizing
    $$
    Err(f) = \mathbb{E}_{\X{X},\y{Y}}\{ L(\y{Y}, \model{f}(\X{X})) \}.
    $$
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Decision trees \citep{breiman:1984}}
    \begin{columns}
        \begin{column}{0.3\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-partition-d.pdf}
            \end{figure}
        \end{column}
        \begin{column}{0.7\textwidth}
            \begin{figure}
            \includegraphics[width=\textwidth]{./figures/tree-simple.pdf}
            \end{figure}
        \end{column}
    \end{columns}

{\scriptsize
\begin{algorithmic}
\Function{BuildDecisionTree}{${\cal L}$}
    \State Create node $t$
    \If{the stopping criterion is met for $t$}
        \State Assign a model to $\widehat{y}_t$
    \Else
        \State Find the split on ${\cal L}$ that maximizes impurity decrease $$s^* = \argmax_{s} i(t) - p_L i(t^s_L) - p_R i(t^s_R)$$
        \State Partition ${\cal L}$ into ${\cal L}_{t_L} \cup {\cal L}_{t_R}$ according to $s^*$
        \State $t_L = \Call{BuildDecisionTree}{{\cal L}_{t_L}}$
        \State $t_R = \Call{BuildDecisionTree}{{\cal L}_{t_R}}$
    \EndIf
    \State \Return $t$
\EndFunction
\end{algorithmic}
}

\end{frame}

\begin{frame}
    \begin{center}
        (Demo)
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Composability of decision trees}

    Decision trees can be used to solve several machine learning tasks by
    swapping the impurity and leaf model functions:

    \vspace{0.5cm}

    \begin{block}{0-1 loss (classification)}
    $\widehat{y}_t = \argmax_{c \in {\cal Y}} p(c|t)$, $i(t) = \text{entropy}(t)$ or $i(t) = \text{gini}(t)$
    \end{block}

    \begin{block}{Mean squared error (regression)}
    $\widehat{y}_t = \text{mean}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} (y - \widehat{y}_t)^2$
    \end{block}

    \begin{block}{Least absolute deviance (regression)}
    $\widehat{y}_t = \text{median}(y|t)$, $i(t) = \frac{1}{N_t} \sum_{\mathbf{x}, y \in {\cal L}_t} |y - \widehat{y}_t|$
    \end{block}

    \begin{block}{Density estimation}
    $\widehat{y}_t = {\cal N}(\mu_t, \Sigma_t)$, $i(t) = \text{differential entropy}(t)$
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Sample weights}

    Sample weights can be accounted for by adapting the impurity and leaf model functions.

    \vspace{0.5cm}

    \begin{block}{Weighted mean squared error}
    $\widehat{y}_t = \frac{1}{\sum_{w} w} \sum_{\mathbf{x}, y, w \in {\cal L}_t} w y$\\
    $i(t) = \frac{1}{\sum_{w} w}  \sum_{\mathbf{x}, y, w \in {\cal L}_t} w (y - \widehat{y}_t)^2$
    \end{block}

    \vspace{0.5cm}

    Weights are assumed to be {\color{red}non-negative} since these quantities may otherwise be undefined. (E.g., what if $\sum_{w} w < 0$?)

    % show an example for mse
    % what about negative sample weights

\end{frame}

\begin{frame}[fragile]
    \frametitle{\texttt{sklearn.tree}}

{\scriptsize
\begin{pythoncode}
# Fit a decision tree
from sklearn.tree import DecisionTreeRegressor
estimator = DecisionTreeRegressor(criterion="mse",   # Set i(t) function
                                  max_leaf_nodes=5)
estimator.fit(X_train, y_train)

# Predict target values
y_pred = estimator.predict(X_test)

# MSE on test data
from sklearn.metrics import mean_squared_error
score = mean_squared_error(y_test, y_pred)
>>> 0.572049826453
\end{pythoncode}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Visualize and interpret}

{\footnotesize
\begin{pythoncode}
# Display tree
from sklearn.tree import export_graphviz
export_graphviz(estimator, out_file="tree.dot",
                feature_names=feature_names)
\end{pythoncode}
}

\begin{figure}
\includegraphics[width=0.8\textwidth]{./figures/wine-tree.png}
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Strengths and weaknesses of decision trees}

  \begin{itemize}
    \item {\color{blue} Non-parametric} model, proved to be
          {\color{blue} consistent}.

    \vspace{0.25cm}

    \item Support {\color{blue} heterogeneous} data (continuous, ordered or
          categorical variables).

    \vspace{0.25cm}

    \item {\color{blue}Flexibility} in loss functions (but choice is
          {\color{red}limited}).

    \vspace{0.25cm}

    \item {\color{blue} Fast} to train, {\color{blue} fast} to predict.
        \begin{itemize}
            \item In the average case, complexity of training is $\Theta(pN\log^2 N)$.
        \end{itemize}

    \vspace{0.25cm}

    \item Easily {\color{blue} interpretable}.

    \vspace{0.25cm}

    \item {\color{blue} Low bias}, but usually {\color{red} high variance}\\
        \begin{itemize}
            \item Solution: Combine the predictions of several randomized
                  trees into a single model.
        \end{itemize}
  \end{itemize}
\end{frame}


% Forests and boosting ========================================================

\section{Forests of randomized trees}

\begin{frame}{Bias-variance decomposition in regression}

{\bf Theorem.} For the {\it squared error loss}, the bias-variance decomposition of the expected
generalization error at $X=\mathbf{x}$ is
\begin{equation*}
\mathbb{E}_{\cal L} \{ Err(f_{\cal L}(\mathbf{x})) \} = \text{noise}(\mathbf{x}) + \text{bias}^2(\mathbf{x}) + \text{var}(\mathbf{x})
\end{equation*}
where
\begin{align*}
\text{noise}(\mathbf{x}) &= Err(f_B(\mathbf{x})), \\
\text{bias}^2(\mathbf{x}) &= (f_B(\mathbf{x}) - \mathbb{E}_{\cal L} \{ f_{\cal L}(\mathbf{x}) \} )^2, \\
\text{var}(\mathbf{x}) &= \mathbb{E}_{\cal L} \{ (\mathbb{E}_{\cal L} \{ f_{\cal L}(\mathbf{x}) \} - f_{\cal L}(\mathbf{x}))^2 \}.
\end{align*}

\end{frame}

\begin{frame}{Bias-variance decomposition}
\begin{figure}
\includegraphics[scale=0.7]{./figures/bias-variance-darts.jpg}
\end{figure}
\end{frame}

\begin{frame}{Diagnosing the generalization error of a decision tree}

\begin{itemize}
\item (Residual error: Lowest achievable error, independent of $f_{\cal L}$.)
\item Bias: Decision trees usually have {\color{blue} low bias}.
\item Variance: They often suffer from {\color{red} high variance}.
\end{itemize}

\begin{itemize}
\item Solution: {\it Combine the predictions of several randomized trees into a single model.}
\end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Random Forests \citep{breiman:2001,geurts:2006}}

    \begin{figure}
        \includegraphics[scale=0.5]{./figures/forest.pdf}
    \end{figure}

    Randomization

    \vspace{0.1cm}

    {\scriptsize
    \begin{tabular}{lll}

    \textbullet\hspace*{0.1cm} Bootstrap samples & \multirow{2}{*}{{\LARGE \}} {\color{blue} Random Forests}} & \\
    \textbullet\hspace*{0.1cm} Random selection of $K \leq p$ split variables && \multirow{2}{*}{{\LARGE \}} {\color{blue} Extra-Trees}} \\
    \textbullet\hspace*{0.1cm} Random selection of the threshold  & \\
    \end{tabular}}
\end{frame}


\begin{frame}{Bias-variance decomposition}

{\bf Theorem.}
For the squared error loss, the bias-variance decomposition of the expected
generalization error $\mathbb{E}_{\cal L} \{ Err( \psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x}))
\}$ at $X=\mathbf{x}$ of an ensemble of $M$ randomized models $f_{{\cal L},\theta_m}$ is
\begin{equation*}
\mathbb{E}_{\cal L} \{ Err(\psi_{{\cal L},\theta_1,\dots,\theta_M}(\mathbf{x})) \} = \text{noise}(\mathbf{x}) + \text{bias}^2(\mathbf{x}) + \text{var}(\mathbf{x}),
\end{equation*}
where
\begin{align*}
\text{noise}(\mathbf{x}) &= Err(f_B(\mathbf{x})), \\
\text{bias}^2(\mathbf{x}) &= (f_B(\mathbf{x}) - \mathbb{E}_{{\cal L},\theta} \{ f_{{\cal L},\theta}(\mathbf{x}) \} )^2, \\
\text{var}(\mathbf{x}) &= \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x}).
\end{align*}

and where $\rho(\mathbf{x})$ is the Pearson correlation coefficient between
the predictions of two randomized trees built on the same learning set.

\end{frame}

\begin{frame}{Interpretation of $\rho(\mathbf{x})$ {\scriptsize (Louppe, 2014)}}

{\bf Theorem.} $\rho(\mathbf{x}) = \frac{\mathbb{V}_{\cal L} \{ \mathbb{E}_{\theta|{\cal L}} \{ f_{{\cal L},\theta}(\mathbf{x}) \} \}}{\mathbb{V}_{\cal L} \{ \mathbb{E}_{\theta|{\cal L}} \{ f_{{\cal L},\theta}(\mathbf{x}) \} \} + \mathbb{E}_{\cal L} \{ \mathbb{V}_{\theta|{\cal L}} \{ f_{{\cal L},\theta}(\mathbf{x}) \} \}}$

\vspace{1cm}

In other words, it is the ratio between
\begin{itemize}
\item the variance due to the learning set and
\item the total variance, accounting for random effects due to both the
  learning set and the random perburbations.
\end{itemize}

\bigskip

$\rho(\mathbf{x}) \to 1$ when variance is mostly due to the learning set; \\
$\rho(\mathbf{x}) \to 0$ when variance is mostly due to the random perturbations;\\
$\rho(\mathbf{x}) \geq 0$.


\end{frame}


\begin{frame}{Diagnosing the error of random forests \citep{louppe2014understanding}}

\begin{itemize}
\item Bias: {\color{blue} Identical} to the bias of a single randomized tree.
\item Variance: $\text{var}(\mathbf{x}) = \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x}) + \frac{1 - \rho(\mathbf{x})}{M} \sigma^2_{{\cal L},\theta}(\mathbf{x})$\\
As $M \to \infty$, {\color{red} $\text{var}(\mathbf{x}) \to \rho(\mathbf{x}) \sigma^2_{{\cal L},\theta}(\mathbf{x})$}
  \begin{itemize}
    \item The stronger the randomization, $\rho(\mathbf{x}) \to 0$, $\text{var}(\mathbf{x}) \to 0$.
    \item The weaker the randomization, $\rho(\mathbf{x}) \to 1$, $\text{var}(\mathbf{x}) \to \sigma^2_{{\cal L},\theta}(\mathbf{x})$
  \end{itemize}
\end{itemize}

\vspace{1cm}

{\bf Bias-variance trade-off.} Randomization increases bias but makes it
possible to reduce the variance of the corresponding ensemble model. The crux
of the problem is to {\color{red} find the right trade-off}.

\end{frame}


\begin{frame}[fragile]
\frametitle{Tuning randomization in \texttt{sklearn.ensemble}}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.cross_validation import ShuffleSplit
from sklearn.learning_curve import validation_curve

# Validation of max_features, controlling randomness in forests
param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]

_, test_scores = validation_curve(
    RandomForestRegressor(n_estimators=100, n_jobs=-1), X, y,
    cv=ShuffleSplit(n=len(X), n_iter=10, test_size=0.25),
    param_name="max_features", param_range=param_range,
    scoring="mean_squared_error")
test_scores_mean = np.mean(-test_scores, axis=1)
plt.plot(param_range, test_scores_mean, label="RF", color="g")

_, test_scores = validation_curve(
    ExtraTreesRegressor(n_estimators=100, n_jobs=-1), X, y,
    cv=ShuffleSplit(n=len(X), n_iter=10, test_size=0.25),
    param_name="max_features", param_range=param_range,
    scoring="mean_squared_error")
test_scores_mean = np.mean(-test_scores, axis=1)
plt.plot(param_range, test_scores_mean, label="ETs", color="r")
\end{pythoncode}
}
\end{frame}


\begin{frame}[fragile]
    \frametitle{Tuning randomization in \texttt{sklearn.ensemble}}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{./figures/randomness.png}
    \end{figure}

    \begin{center}
        Best-tradeoff: ExtraTrees, for \texttt{max\_features=6}.
    \end{center}
\end{frame}




\begin{frame}
  \frametitle{Strengths and weaknesses of forests}

  \begin{itemize}
    \item One of the best off-the-self learning algorithm, requiring almost
          no tuning.

    \vspace{0.25cm}

    \item {\color{blue}Fine control} of bias and variance through averaging
          and randomization, resulting in {\color{blue} better performance}.

    \vspace{0.25cm}

    \item Moderately fast to train and to predict.
        \begin{itemize}
            \item $\Theta(MK\widetilde{N}\log^2 \widetilde{N})$ for RFs (where $\widetilde{N} = 0.632N$)
            \item $\Theta(MKN\log N)$ for ETs
        \end{itemize}

    \vspace{0.25cm}

    \item Embarrassingly {\color{blue} parallel} (use \texttt{n\_jobs}).

    \vspace{0.25cm}

    \item {\color{red} Less interpretable} than decision trees.
  \end{itemize}
\end{frame}





% Forests and boosting ========================================================

\section{Boosting}

\begin{frame}
  \frametitle{AdaBoost \citep{freund1995desicion}}

  Can we build a strong learner from multiple weak learners?

  \begin{itemize}
     \item Ensemble: each member is an expert on the errors of its predecessor;
     \item Iteratively re-weights training examples based on errors.
  \end{itemize}


  \begin{figure}
    \includegraphics[width=\textwidth,trim=0 0 0 10, clip=true]{figures/adaboost.pdf}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{AdaBoost \citep{freund1995desicion}}

  \begin{center}
      {\Huge Huge success}
  \end{center}

  \begin{itemize}
     \item Viola-Jones Face Detector (2001)
     \begin{figure}
        \includegraphics[scale=0.3]{figures/viola-jones.png}
      \end{figure}
     \item Freund \& Schapire won the Gödel prize (2003)
  \end{itemize}

\end{frame}



\begin{frame}
  \frametitle{Gradient Boosted Machines \citep{friedman:2001}}

  Statistical view of boosting:

  \begin{itemize}
      \item Ultimately, we are interested in finding
      $$f^* = \arg \min_f \mathbb{E}_{X,Y} [L(Y,f(X))]$$
      for an arbitrary loss $L$;
      \item Under the boosting assumption that $f^*$ can be expressed as an additive model $\hat{f}$ of the form
      $$\hat{f} =  c + \sum_{m=1}^M \rho_m h_m.$$
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Greedy approximation}

  \begin{itemize}
      \item The ensemble is built in a forward stagewise fashion:
      $$f_m = f_{m-1} + \rho_m h_m$$
      \item In accordance with the empirical risk minimization principle,
      the weak learner $\rho_m h_m$ is chosen to minimize the
      loss $L$ on the training set, given the current model $f_{m-1}$:
      \begin{align*}
          f_m & = f_{m-1} + \arg \min_{\rho,h} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \rho h(x_i)) \\
          f_0 &= \arg \min_c\sum_{i=1}^N L(y_i, c)
      \end{align*}
      \item Solving for $\rho, h$ at each step for an arbitrary loss $L$
      and/or abitrary family ${\cal H}$  is a difficult optimization problem.
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Steepest descent in function space}

 \begin{columns}
     \begin{column}{.75\textwidth}

         Solution: approximate using steepest descent in function space, by
             choosing a function $h$ that is the most parallel to the negative gradient
             $\{ g_m(x_i) \}_{i=1}^N$ on the training data:
             \begin{align*}
                 g_m(x) &= \mathbb{E}_{Y|X=x} \left[ \frac{\partial L(Y,f(x))}{\partial f(x)} \right]_{f(x) = f_{m-1}(x)} \\
                 h_m &= \arg \min_h \sum_{i=1}^N (-g_m(x_i) - h(x_i))^2 \\
                 \rho_m &= \arg \min_\gamma L(y_i, f_{m-1}(x_i) + \rho h_m(x_i))
             \end{align*}
             where the second equation is solved using a standard regression algorithm (e.g. decision trees)
             and the third using line search.

     \end{column}
     \begin{column}{.25\textwidth}
         \begin{figure}
            \centering
            \includegraphics[width=2.\textwidth]{figures/steepest.png}
         \end{figure}
     \end{column}
 \end{columns}

\end{frame}

\begin{frame}
    \frametitle{In short}
    \begin{itemize}
    \item GBM fits an additive model of the form
    $\hat{f} =  c + \sum_{m=1}^M \rho_m h_m.$

    \item Where $h_m$ are regression trees approximating
          the negative gradient step
          $-\left[ \frac{\partial L(Y,f(x))}{\partial f(x)} \right]_{f(x) = f_{m-1}(x)}$.

    \item For $L(Y,f(X)) = (Y-f(X))^2$, regression trees $h_m$ approximate the residuals $y_i -f_{m-1}(x_i)$.
    \end{itemize}

  \begin{figure}
     \centering
     \includegraphics[width=\textwidth]{./figures/residual_fitting_2.pdf}
  \end{figure}
\end{frame}

\begin{frame}[fragile]{Example}

    \begin{minted}[fontsize=\small]{python}
from sklearn.ensemble import GradientBoostingRegressor
est = GradientBoostingRegressor(n_estimators=100).fit(X, y)
    \end{minted}

   \begin{figure}
     \centering
       \includegraphics[width=0.8\textwidth]{figures/func_approx3.pdf}
   \end{figure}

\end{frame}


\begin{frame}[fragile]{Model complexity and overfitting}

   \begin{figure}
     \centering
       \includegraphics[width=0.5\textwidth]{figures/overfitting.pdf}
   \end{figure}

   % Overlay: GBRT provides a number of means to control overfitting

     GBM provides a number of knobs to control overfitting:
     \begin{itemize}
       \item Shrinkage
       \item Tree structure
       \item Stochastic Gradient Boosting
     \end{itemize}
\end{frame}


\begin{frame}{Regularization: shrinkage}

    \begin{itemize}
      \item Slow learning by shrinking tree predictions with $0 < \text{learning\_rate} <= 1$
      \item Lower \texttt{learning\_rate} requires higher \texttt{n\_estimators}
    \end{itemize}

   \begin{figure}
     \centering
       \includegraphics[width=0.75\textwidth]{figures/regularization-learning-rate.pdf}
   \end{figure}

\end{frame}


\begin{frame}{Regularization: tree structure}

    \begin{itemize}
      \item The \texttt{max\_depth} of the trees controls the degree of features interactions
      \item Use \texttt{min\_samples\_leaf} to have a sufficient nr. of samples per leaf.
    \end{itemize}

   \begin{figure}
     \centering
       \includegraphics[width=0.75\textwidth]{figures/regularization-tree-structure.pdf}
   \end{figure}

\end{frame}



\begin{frame}{Regularization: stochastic gradient boosting}

    \begin{itemize}
      \item Samples: random subset of the training set (\texttt{subsample})
      \item Features: random subset of features (\texttt{max\_features})
      \item Improved accuracy -- reduced runtime
    \end{itemize}

   \begin{figure}
     \centering
       \includegraphics[width=0.75\textwidth]{figures/regularization-subsampling.pdf}
   \end{figure}

\end{frame}

\begin{frame}[fragile]{Hyperparameter tuning}

\begin{enumerate}
  \item Set \texttt{n\_estimators} as high as possible (eg. $3000$)
  \vspace{0.2cm}
  \item Tune hyperparameters via grid search.
  %\vspace{0.3cm}

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.grid_search import GridSearchCV
param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],
              'max_depth': [4, 6],
              'min_samples_leaf': [3, 5, 9, 17],
              'max_features': [1.0, 0.3, 0.1]}
est = GradientBoostingRegressor(n_estimators=3000)
gs_cv = GridSearchCV(est, param_grid).fit(X, y)
# best hyperparameter setting
gs_cv.best_params_
\end{minted}
  \vspace{0.2cm}
  \item Finally, set \texttt{n\_estimators} even higher and tune \texttt{learning\_rate}.

\end{enumerate}
\end{frame}



\begin{frame}
  \frametitle{Strengths and weaknesses of Boosting}

  \begin{itemize}
        \item Often {\color{blue} more accurate} than random forests.

         \vspace{0.25cm}

        \item {\color{blue} Flexible framework}, that can adapt to arbitrary
              loss functions.

        \vspace{0.25cm}

        \item Fine control of under/overfitting through {\color{blue}
              regularization} (e.g., learning rate, subsampling,
              tree structure, penalization term in the loss function, etc).

        \vspace{0.25cm}

        \item {\color{red} Careful tuning} required.

        \vspace{0.25cm}

        \item {\color{red} Slow} to train, {\color{blue} fast} to predict.
  \end{itemize}
\end{frame}


% Interpretation ===============================================================

\section{Interpreting tree-based models}

\begin{frame}[fragile]
  \frametitle{Variable selection/ranking/exploration}

  Tree-based models come with built-in methods for variable selection, ranking
  or exploration.

  \vspace{0.5cm}

  The main goals are:
  \begin{itemize}
  \item To reduce training times;
  \item To enhance generalisation by reducing overfitting;
  \item To uncover relations between variables and ease model interpretation.
  \end{itemize}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Variable importances}

{\scriptsize
\begin{pythoncode}
# Variable importances with Random Forest, default parameters
forest = RandomForestRegressor(n_estimators=10000, n_jobs=-1).fit(X, y)
forest.feature_importances_
\end{pythoncode}
}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Variable importances}

\begin{figure}
\includegraphics[width=0.95\textwidth]{./figures/importances.png}
\end{figure}

\begin{center}
Importances are measured only through the eyes of the model.
{\color{red}They may not tell the entire nor the same story!} \citep{louppe:2013}
\end{center}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Partial dependence plots}

Relation between the response $Y$ and a subset of features, marginalized over all other features.

\vspace{0.25cm}

{\scriptsize
\begin{pythoncode}
from sklearn.ensemble.partial_dependence import plot_partial_dependence
plot_partial_dependence(gbrt, X,
                        features=[1, 10], feature_names=feature_names)
\end{pythoncode}
}

    \begin{figure}
       \centering
       \includegraphics[width=0.9\textwidth]{./figures/dependence.png}
    \end{figure}
\end{frame}








\begin{frame}[plain,noframenumbering]
    \frametitle{References}
    {\footnotesize
    \bibliographystyle{apalike}
    \bibliography{biblio}}
\end{frame}

\end{document}
